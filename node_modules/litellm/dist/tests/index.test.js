"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
const mockCreate = jest.fn();
jest.mock('openai', () => {
    return jest.fn().mockImplementation(() => {
        return {
            chat: {
                completions: {
                    create: mockCreate,
                },
            },
        };
    });
});
jest.mock('@anthropic-ai/sdk', () => {
    return jest.fn().mockImplementation(() => {
        return {
            completions: {
                create: mockCreate,
            },
        };
    });
});
const sdk_1 = __importDefault(require("@anthropic-ai/sdk"));
const src_1 = require("../src");
describe('litellm', () => {
    describe('openai', () => {
        it.each([
            { model: 'gpt-3.5-turbo', stream: true },
            { model: 'gpt-4-32k-0613', stream: false },
        ])('support using openai chat models with and without streaming', ({ model, stream }) => __awaiter(void 0, void 0, void 0, function* () {
            const params = { model: model, messages: [], stream };
            yield (0, src_1.completion)(params);
            expect(mockCreate).toHaveBeenCalledWith(params);
        }));
    });
    describe('anthropic', () => {
        it('supports using anthropic models without streaming', () => __awaiter(void 0, void 0, void 0, function* () {
            const params = {
                model: 'claude-2',
                messages: [
                    {
                        content: 'How are you',
                        role: 'user',
                    },
                ],
                stream: false,
            };
            const expectedPrompt = `${sdk_1.default.HUMAN_PROMPT} ${params.messages[0].content}${sdk_1.default.AI_PROMPT}`;
            mockCreate.mockResolvedValueOnce({
                completion: 'response text',
            });
            const result = yield (0, src_1.completion)(params);
            const expectedParams = {
                model: 'claude-2',
                prompt: expectedPrompt,
            };
            expect(mockCreate).toHaveBeenCalledWith(expect.objectContaining(expectedParams));
            expect(result).toMatchObject({
                choices: [
                    {
                        message: {
                            content: 'response text',
                        },
                    },
                ],
            });
        }));
    });
});
